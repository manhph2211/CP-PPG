{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 2em;\">\n",
    "    Welcome to CP-PPG!\n",
    "</div>\n",
    "\n",
    "### Install the requirements\n",
    "\n",
    "```bash\n",
    "conda create -n ppg python=3.11\n",
    "conda activate ppg\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Experiment Tool\n",
    "\n",
    "- This is an optional step if you want to track your experiments using comet.ml platform.\n",
    "\n",
    "- Create comet account on the official website. Once you have an account, click to profile to get the API key.\n",
    "\n",
    "- Create a new file inside `config` folder, name it `experiment_apikey.txt`. Then just paste the api key into this file and save it.\n",
    "\n",
    "\n",
    "### Training \n",
    "\n",
    "- Before training, make sure to modify your desired configs in `config.yml` inside the `config` folder. I already set the suitable default config so you can follow that. \n",
    "\n",
    "- To train the model, you can run the following command, comet will log all the necessary metrics and results during training so you can check them in your workspace at the official website. \n",
    "\n",
    "```bash\n",
    "python src/experiments/tools/train.py\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"configs/config.yml\", 'r') as yaml_file:\n",
    "    data = yaml.safe_load(yaml_file)\n",
    "cfgs = yaml.dump(data, default_flow_style=False, indent=4)\n",
    "\n",
    "print(cfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_data_folder(data_folder_path):\n",
    "    subject_folders = [f for f in os.listdir(data_folder_path) if os.path.isdir(os.path.join(data_folder_path, f))]\n",
    "\n",
    "    for subject_folder in subject_folders:\n",
    "        subject_path = os.path.join(data_folder_path, subject_folder)\n",
    "\n",
    "        processed_folder_path = os.path.join(subject_path, 'preprocessed')\n",
    "        os.makedirs(processed_folder_path, exist_ok=True)\n",
    "\n",
    "        csv_files = [f for f in os.listdir(subject_path) if f.endswith('.csv')]\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            source_path = os.path.join(subject_path, csv_file)\n",
    "            destination_path = os.path.join(processed_folder_path, csv_file)\n",
    "            shutil.move(source_path, destination_path)\n",
    "            \n",
    "process_data_folder(\"data/v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from src.utils.utils import get_peaks_info, plot_wavform, plot_peaks, plot_metrics, get_config, plot_two_signal, get_feet\n",
    "from src.utils.preprocess import extract_window_segments\n",
    "from src.utils.preprocess import moving_average, read_processed_signal\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "cfgs = get_config()\n",
    "csv_file = \"data/v2/subject1009/preprocessed/3_preprocessed_1009.csv\"\n",
    "\n",
    "ppg_in, ppg_ref, pressure = read_processed_signal(csv_file, cfgs)\n",
    "fig, axes = plt.subplots(1, 1, figsize = (24, 10))\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\",\"#f4cae4\",\"#FCD5B5\",\"#EDE2D5\"]\n",
    "\n",
    "normed_ref = ppg_ref[1020:1100].reshape(-1) / np.max(ppg_ref[1020:1145])\n",
    "axes.plot(normed_ref, color='orange', linewidth = 3)\n",
    "systolic_peak_index = np.argmax(normed_ref)\n",
    "x_values = np.arange(0, len(normed_ref))\n",
    "feet = get_feet(normed_ref)\n",
    "axes.fill_between(x_values[feet[0]:systolic_peak_index+1], normed_ref[feet[0]:systolic_peak_index+1], color=flatui[-1], alpha=0.5, label=\"Systolic Area (SA)\")\n",
    "axes.fill_between(x_values[systolic_peak_index:feet[1]], normed_ref[systolic_peak_index:feet[1]], color=flatui[-2], alpha=0.5, label=\"Diastolic Area (DA)\")\n",
    "\n",
    "legend = axes.legend(fontsize=30, handles=[\n",
    "    mpatches.Patch(color=flatui[-1], alpha=0.6, label='Systolic Area (SA)'),\n",
    "    mpatches.Patch(color=flatui[-2], alpha=0.6, label='Diastolic Area (DA)')],\n",
    "    loc='upper left', bbox_to_anchor=(0.6, 0.6))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_segments, ref_segments = extract_window_segments(csv_file, cfgs, window_size=5*100)\n",
    "print(src_segments.shape, ref_segments.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(src_segments[5].reshape(-1))\n",
    "plt.title('Source Segment')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ref_segments[5].reshape(-1))\n",
    "plt.title('Reference Segment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_peaks(ref_segments[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.feature import extract_feat_cycle\n",
    "from src.utils.preprocess import cycle_helper\n",
    "from src.utils.utils import standardize\n",
    "\n",
    "breakdown_seg_in, breakdown_seg_ref, _ = cycle_helper(standardize(src_segments[4].reshape(-1)), standardize(ref_segments[4].reshape(-1)), return_cycles=True)\n",
    "plot_wavform(breakdown_seg_in[2], breakdown_seg_ref[2])\n",
    "feat_name, feats, dia_feat_name, dia_feats, valid = extract_feat_cycle(breakdown_seg_ref, fs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = {key: value for key, value in zip(feat_name, feats)}\n",
    "print(feat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from src.utils.classification import classify\n",
    "from src.utils.enrichment import PPGTransform\n",
    "from src.utils.utils import get_peaks_info, plot_wavform, plot_peaks, plot_metrics, get_config, plot_two_signal, standardize\n",
    "\n",
    "transform = PPGTransform()\n",
    "signal_segment = standardize(src_segments[22:23].reshape(-1))\n",
    "transformed_signal = transform.convert(signal_segment)\n",
    "plot_two_signal(signal_segment, transformed_signal) # Before and after augmnentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader.dataset import PPGDataset, get_loader\n",
    "from src.utils.utils import get_config, plot_wavform, read_json, write_json, plot_subject_distribution, standardize\n",
    "from src.utils.prepare import DataHandler\n",
    "\n",
    "cfgs = get_config()\n",
    "dh = DataHandler(cfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.custom_data_hanlder(\"8s\")\n",
    "dh.train_val_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.cpppg import Generator\n",
    "from src.utils.utils import get_config\n",
    "import torch\n",
    "\n",
    "cfgs = get_config()\n",
    "\n",
    "model = Generator(cfgs)\n",
    "x = torch.randn(1, 1, 8 * 128)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.cpppg import Discriminator\n",
    "from src.utils.utils import get_config\n",
    "import torch\n",
    "\n",
    "cfgs = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Discriminator(cfgs)\n",
    "x = torch.randn(1, 1, 5*128) \n",
    "y = model(x)\n",
    "print(y.shape)\n",
    "from torchsummary import summary\n",
    "summary(model, (1, 8*128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "from src.trainer.engine import Trainer\n",
    "from src.trainer.adverarial_engine import AdversarialTrainer\n",
    "from src.utils.utils import get_config\n",
    "from configs.seed import *\n",
    "\n",
    "cfgs = get_config()\n",
    "\n",
    "with open('configs/experiment_apikey.txt','r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "tracking = Experiment(\n",
    "    api_key = api_key,\n",
    "    project_name = \"CP-PPG Project\",\n",
    "    workspace = \"maxph2211\",\n",
    ")\n",
    "tracking.log_parameters(cfgs)\n",
    "if cfgs['train']['adversarial']:\n",
    "    trainer = AdversarialTrainer(tracking, cfgs)\n",
    "else:\n",
    "    trainer = Trainer(tracking, cfgs)\n",
    "    \n",
    "# trainer.training_experiment() \n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.dataloader.dataset import PPGDataset\n",
    "from src.experiments.tools.test import Inference\n",
    "from src.utils.utils import get_config, standardize\n",
    "from tqdm import tqdm\n",
    "\n",
    "cfgs = get_config()\n",
    "infer_tool = Inference(cfgs)\n",
    "test_dataset = PPGDataset(cfgs, data_path=\"\")\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=cfgs['data']['batch_size'],\n",
    "                        num_workers=cfgs['data']['num_workers']\n",
    ")\n",
    "\n",
    "src_signals, out_signals, ref_signals = [], [], []\n",
    "\n",
    "for src_signal, ref_signal, _, _ in tqdm(test_loader):\n",
    "    output_batch_tensor = infer_tool.infer(src_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from src.experiments.tools.test import Inference\n",
    "from src.utils.utils import get_config\n",
    "import joblib\n",
    "import numpy as np \n",
    "### v1\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "inference = Inference(get_config())\n",
    "\n",
    "@app.route(\"/enhance\", methods=[\"POST\"])\n",
    "def enhance_signal():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        corrupted_signal = data.get(\"corrupted_signal\", [])\n",
    "        if not corrupted_signal:\n",
    "            return jsonify({\"error\": \"Invalid or missing 'corrupted_signal' field.\"}), 400\n",
    "\n",
    "        reconstructed_signal = inference.infer(corrupted_signal)\n",
    "\n",
    "        return jsonify({\"reconstructed_signal\": reconstructed_signal.tolist()}), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/preprocess', methods=['POST'])\n",
    "def preprocess_signal():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        signal = data.get('signal', [])\n",
    "        signal = np.array(signal)\n",
    "        normalized_signal = standardize(signal.reshape(-1,1)).reshape(signal.shape)\n",
    "        return jsonify({'normalized_signal': normalized_signal.tolist()}), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "app.run(host=\"0.0.0.0\", debug=True, port=8085)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppg1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
